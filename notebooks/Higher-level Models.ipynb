{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain higher classification accuracy, we implemented neural networks, which we did not learn in the class. Multi-layer perceptrons neural network is a supervised method, and is very powerful in classifying Alzheimer's disease as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv(\"data/ADNIMERGE_train.csv\")\n",
    "df_test = pd.read_csv(\"data/ADNIMERGE_test.csv\")\n",
    "X_train = df_train.drop(['RID', 'DX_bl'], axis=1).copy()\n",
    "y_train = df_train['DX_bl'].copy()\n",
    "X_test = df_test.drop(['RID', 'DX_bl'], axis=1).copy()\n",
    "y_test = df_test['DX_bl'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to help compare the accuracy of models\n",
    "def score(model, X_train, y_train, X_test, y_test):\n",
    "    train_acc = model.score(X_train,y_train)\n",
    "    test_acc = model.score(X_test,y_test)\n",
    "    test_class0 = model.score(X_test[y_test==0], y_test[y_test==0])\n",
    "    test_class1 = model.score(X_test[y_test==1], y_test[y_test==1])\n",
    "    test_class2 = model.score(X_test[y_test==2], y_test[y_test==2])\n",
    "    return pd.Series([train_acc, test_acc, test_class0, test_class1, test_class2],\n",
    "                    index = ['Train accuracy', 'Test accuracy', \n",
    "                             \"Test accuracy CN\", \"Test accuracy CI\", \"Test accuracy AD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_standardize = [\n",
    "    c for c in X_train.columns \n",
    "    if (not c.startswith('PT')) \\\n",
    "        or (c=='PTEDUCAT') or (c=='PTAGE')]\n",
    "\n",
    "X_train_std = X_train.copy()\n",
    "X_test_std = X_test.copy()\n",
    "for c in cols_standardize:\n",
    "    col_mean = np.mean(X_train[c])\n",
    "    col_sd = np.std(X_train[c])\n",
    "    if col_sd > (1e-10)*col_mean:\n",
    "        X_train_std[c] = (X_train[c]-col_mean)/col_sd\n",
    "        X_test_std[c] = (X_test[c]-col_mean)/col_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(621, 75)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTAGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>PTRACCAT_Asian</th>\n",
       "      <th>PTRACCAT_Black</th>\n",
       "      <th>PTRACCAT_Hawaiian/Other_PI</th>\n",
       "      <th>PTRACCAT_More_than_one</th>\n",
       "      <th>PTRACCAT_Unknown</th>\n",
       "      <th>PTRACCAT_White</th>\n",
       "      <th>PTETHCAT_Not_Hisp/Latino</th>\n",
       "      <th>PTMARRY_Married</th>\n",
       "      <th>PTMARRY_Never_married</th>\n",
       "      <th>PTMARRY_Widowed</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>CSF_ABETA</th>\n",
       "      <th>CSF_TAU</th>\n",
       "      <th>CSF_PTAU</th>\n",
       "      <th>FDG</th>\n",
       "      <th>FDG_slope</th>\n",
       "      <th>AV45</th>\n",
       "      <th>AV45_slope</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>ADAS13_slope</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>MMSE_slope</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_immediate_slope</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_learning_slope</th>\n",
       "      <th>RAVLT_forgetting</th>\n",
       "      <th>RAVLT_forgetting_slope</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>RAVLT_perc_forgetting_slope</th>\n",
       "      <th>MOCA</th>\n",
       "      <th>MOCA_slope</th>\n",
       "      <th>EcogPtMem</th>\n",
       "      <th>EcogPtMem_slope</th>\n",
       "      <th>EcogPtLang</th>\n",
       "      <th>EcogPtLang_slope</th>\n",
       "      <th>EcogPtVisspat</th>\n",
       "      <th>EcogPtVisspat_slope</th>\n",
       "      <th>EcogPtPlan</th>\n",
       "      <th>EcogPtPlan_slope</th>\n",
       "      <th>EcogPtOrgan</th>\n",
       "      <th>EcogPtOrgan_slope</th>\n",
       "      <th>EcogPtDivatt</th>\n",
       "      <th>EcogPtDivatt_slope</th>\n",
       "      <th>EcogSPMem</th>\n",
       "      <th>EcogSPMem_slope</th>\n",
       "      <th>EcogSPLang</th>\n",
       "      <th>EcogSPLang_slope</th>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <th>EcogSPVisspat_slope</th>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <th>EcogSPPlan_slope</th>\n",
       "      <th>EcogSPOrgan</th>\n",
       "      <th>EcogSPOrgan_slope</th>\n",
       "      <th>EcogSPDivatt</th>\n",
       "      <th>EcogSPDivatt_slope</th>\n",
       "      <th>FAQ</th>\n",
       "      <th>FAQ_slope</th>\n",
       "      <th>Ventricles</th>\n",
       "      <th>Ventricles_slope</th>\n",
       "      <th>Hippocampus</th>\n",
       "      <th>Hippocampus_slope</th>\n",
       "      <th>WholeBrain</th>\n",
       "      <th>WholeBrain_slope</th>\n",
       "      <th>Entorhinal</th>\n",
       "      <th>Entorhinal_slope</th>\n",
       "      <th>Fusiform</th>\n",
       "      <th>Fusiform_slope</th>\n",
       "      <th>MidTemp</th>\n",
       "      <th>MidTemp_slope</th>\n",
       "      <th>ICV</th>\n",
       "      <th>ICV_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.208480</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.852257</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.823084</td>\n",
       "      <td>-1.421715</td>\n",
       "      <td>1.137347</td>\n",
       "      <td>-0.321586</td>\n",
       "      <td>-0.833130</td>\n",
       "      <td>0.811782</td>\n",
       "      <td>1.257634</td>\n",
       "      <td>0.023763</td>\n",
       "      <td>2.140034</td>\n",
       "      <td>0.236256</td>\n",
       "      <td>-2.742373</td>\n",
       "      <td>-0.471665</td>\n",
       "      <td>-0.010026</td>\n",
       "      <td>-0.001926</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>0.028414</td>\n",
       "      <td>-0.018332</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.012471</td>\n",
       "      <td>-0.003150</td>\n",
       "      <td>-0.004052</td>\n",
       "      <td>-0.827740</td>\n",
       "      <td>1.136728</td>\n",
       "      <td>-0.981277</td>\n",
       "      <td>-0.002156</td>\n",
       "      <td>-0.715234</td>\n",
       "      <td>-0.030329</td>\n",
       "      <td>-0.713577</td>\n",
       "      <td>-0.002410</td>\n",
       "      <td>-0.843488</td>\n",
       "      <td>-0.007633</td>\n",
       "      <td>-1.076964</td>\n",
       "      <td>-0.016544</td>\n",
       "      <td>1.929076</td>\n",
       "      <td>-0.088208</td>\n",
       "      <td>-0.844339</td>\n",
       "      <td>0.954666</td>\n",
       "      <td>1.066767</td>\n",
       "      <td>1.048123</td>\n",
       "      <td>2.524016</td>\n",
       "      <td>0.060641</td>\n",
       "      <td>2.287022</td>\n",
       "      <td>0.021430</td>\n",
       "      <td>2.330806</td>\n",
       "      <td>-0.631242</td>\n",
       "      <td>2.934694</td>\n",
       "      <td>0.124173</td>\n",
       "      <td>-0.205919</td>\n",
       "      <td>0.381384</td>\n",
       "      <td>-1.351616</td>\n",
       "      <td>0.022285</td>\n",
       "      <td>-1.761500</td>\n",
       "      <td>-0.567555</td>\n",
       "      <td>-0.820814</td>\n",
       "      <td>-1.269796</td>\n",
       "      <td>-1.426968</td>\n",
       "      <td>0.156847</td>\n",
       "      <td>-2.102069</td>\n",
       "      <td>-0.192827</td>\n",
       "      <td>-1.574482</td>\n",
       "      <td>0.093937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.759714</td>\n",
       "      <td>1</td>\n",
       "      <td>1.376909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.703236</td>\n",
       "      <td>-0.567568</td>\n",
       "      <td>-0.086517</td>\n",
       "      <td>-0.128000</td>\n",
       "      <td>0.602793</td>\n",
       "      <td>2.864460</td>\n",
       "      <td>1.903103</td>\n",
       "      <td>1.199280</td>\n",
       "      <td>-0.210782</td>\n",
       "      <td>-0.364551</td>\n",
       "      <td>0.586715</td>\n",
       "      <td>0.595629</td>\n",
       "      <td>0.202187</td>\n",
       "      <td>-0.189405</td>\n",
       "      <td>0.884055</td>\n",
       "      <td>-0.907178</td>\n",
       "      <td>-0.118596</td>\n",
       "      <td>1.750365</td>\n",
       "      <td>-0.556812</td>\n",
       "      <td>1.770545</td>\n",
       "      <td>0.227228</td>\n",
       "      <td>-0.044392</td>\n",
       "      <td>-0.827740</td>\n",
       "      <td>-0.229483</td>\n",
       "      <td>-1.153377</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.444998</td>\n",
       "      <td>-0.101471</td>\n",
       "      <td>-0.713577</td>\n",
       "      <td>-0.065235</td>\n",
       "      <td>-0.843488</td>\n",
       "      <td>-0.064451</td>\n",
       "      <td>-1.076964</td>\n",
       "      <td>-0.072474</td>\n",
       "      <td>0.573694</td>\n",
       "      <td>-0.738447</td>\n",
       "      <td>-0.106940</td>\n",
       "      <td>-1.151805</td>\n",
       "      <td>0.487637</td>\n",
       "      <td>-1.687565</td>\n",
       "      <td>1.262926</td>\n",
       "      <td>-2.477232</td>\n",
       "      <td>0.148179</td>\n",
       "      <td>-1.526413</td>\n",
       "      <td>0.971872</td>\n",
       "      <td>-1.384981</td>\n",
       "      <td>-0.484392</td>\n",
       "      <td>-0.424658</td>\n",
       "      <td>-0.053348</td>\n",
       "      <td>-0.251084</td>\n",
       "      <td>0.483644</td>\n",
       "      <td>-0.306894</td>\n",
       "      <td>-0.134464</td>\n",
       "      <td>-0.028641</td>\n",
       "      <td>-0.070387</td>\n",
       "      <td>0.188014</td>\n",
       "      <td>0.721399</td>\n",
       "      <td>-0.067438</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.506511</td>\n",
       "      <td>-0.489132</td>\n",
       "      <td>-0.265646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.257208</td>\n",
       "      <td>0</td>\n",
       "      <td>0.607970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.823084</td>\n",
       "      <td>-0.775573</td>\n",
       "      <td>-1.038657</td>\n",
       "      <td>-0.920353</td>\n",
       "      <td>0.572072</td>\n",
       "      <td>0.019993</td>\n",
       "      <td>0.547072</td>\n",
       "      <td>0.023763</td>\n",
       "      <td>-0.798486</td>\n",
       "      <td>-0.710060</td>\n",
       "      <td>0.586715</td>\n",
       "      <td>0.463914</td>\n",
       "      <td>1.183334</td>\n",
       "      <td>1.011032</td>\n",
       "      <td>0.884055</td>\n",
       "      <td>-0.590705</td>\n",
       "      <td>0.280068</td>\n",
       "      <td>-1.497230</td>\n",
       "      <td>-0.495770</td>\n",
       "      <td>-1.722958</td>\n",
       "      <td>0.452935</td>\n",
       "      <td>0.701979</td>\n",
       "      <td>-0.530055</td>\n",
       "      <td>-0.049247</td>\n",
       "      <td>-0.464974</td>\n",
       "      <td>-0.085273</td>\n",
       "      <td>-0.715234</td>\n",
       "      <td>-0.033968</td>\n",
       "      <td>-0.713577</td>\n",
       "      <td>-0.065235</td>\n",
       "      <td>-0.018265</td>\n",
       "      <td>0.055382</td>\n",
       "      <td>-0.053520</td>\n",
       "      <td>0.069781</td>\n",
       "      <td>-1.052764</td>\n",
       "      <td>0.042214</td>\n",
       "      <td>-0.844339</td>\n",
       "      <td>-0.077852</td>\n",
       "      <td>-0.670610</td>\n",
       "      <td>-0.211993</td>\n",
       "      <td>-0.754818</td>\n",
       "      <td>0.111691</td>\n",
       "      <td>-0.824015</td>\n",
       "      <td>-0.212459</td>\n",
       "      <td>-0.930635</td>\n",
       "      <td>0.246899</td>\n",
       "      <td>-0.647205</td>\n",
       "      <td>-0.415515</td>\n",
       "      <td>-0.294977</td>\n",
       "      <td>-0.883418</td>\n",
       "      <td>-1.104796</td>\n",
       "      <td>0.020857</td>\n",
       "      <td>-1.300396</td>\n",
       "      <td>0.310720</td>\n",
       "      <td>0.456478</td>\n",
       "      <td>-0.560840</td>\n",
       "      <td>0.292776</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>-0.650452</td>\n",
       "      <td>0.224140</td>\n",
       "      <td>-1.239633</td>\n",
       "      <td>-0.014198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.521887</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.160970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.823084</td>\n",
       "      <td>-0.445863</td>\n",
       "      <td>0.222762</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.441716</td>\n",
       "      <td>0.850276</td>\n",
       "      <td>-0.548541</td>\n",
       "      <td>-0.049974</td>\n",
       "      <td>-0.994388</td>\n",
       "      <td>-0.282924</td>\n",
       "      <td>0.586715</td>\n",
       "      <td>0.519651</td>\n",
       "      <td>0.730497</td>\n",
       "      <td>0.190865</td>\n",
       "      <td>-0.546632</td>\n",
       "      <td>0.454839</td>\n",
       "      <td>-0.915923</td>\n",
       "      <td>1.164759</td>\n",
       "      <td>-1.032937</td>\n",
       "      <td>0.965802</td>\n",
       "      <td>0.678642</td>\n",
       "      <td>0.204009</td>\n",
       "      <td>0.387783</td>\n",
       "      <td>0.167724</td>\n",
       "      <td>0.223444</td>\n",
       "      <td>0.375527</td>\n",
       "      <td>-0.174781</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>-0.007973</td>\n",
       "      <td>0.191936</td>\n",
       "      <td>0.531879</td>\n",
       "      <td>0.404523</td>\n",
       "      <td>1.652218</td>\n",
       "      <td>0.250482</td>\n",
       "      <td>-0.103997</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>-0.844339</td>\n",
       "      <td>-0.186429</td>\n",
       "      <td>-0.670610</td>\n",
       "      <td>-0.271617</td>\n",
       "      <td>-0.754818</td>\n",
       "      <td>-0.343706</td>\n",
       "      <td>-0.824015</td>\n",
       "      <td>-0.076909</td>\n",
       "      <td>-0.115275</td>\n",
       "      <td>0.065754</td>\n",
       "      <td>-0.484392</td>\n",
       "      <td>-0.473929</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>-0.032439</td>\n",
       "      <td>0.013960</td>\n",
       "      <td>0.020857</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.003749</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>-0.003683</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.015345</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>0.004091</td>\n",
       "      <td>-0.005136</td>\n",
       "      <td>0.004314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.121905</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.160970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.823084</td>\n",
       "      <td>-1.521292</td>\n",
       "      <td>0.516578</td>\n",
       "      <td>0.056582</td>\n",
       "      <td>0.613315</td>\n",
       "      <td>1.113319</td>\n",
       "      <td>1.781157</td>\n",
       "      <td>0.338415</td>\n",
       "      <td>-0.602585</td>\n",
       "      <td>-0.189067</td>\n",
       "      <td>0.586715</td>\n",
       "      <td>0.476561</td>\n",
       "      <td>-0.099704</td>\n",
       "      <td>0.488832</td>\n",
       "      <td>-0.546632</td>\n",
       "      <td>0.189374</td>\n",
       "      <td>-0.118596</td>\n",
       "      <td>0.780831</td>\n",
       "      <td>-0.160039</td>\n",
       "      <td>0.662249</td>\n",
       "      <td>-0.449892</td>\n",
       "      <td>0.202553</td>\n",
       "      <td>-0.133156</td>\n",
       "      <td>0.119957</td>\n",
       "      <td>-0.809176</td>\n",
       "      <td>0.038450</td>\n",
       "      <td>-0.444998</td>\n",
       "      <td>-0.146690</td>\n",
       "      <td>-0.360775</td>\n",
       "      <td>-0.127391</td>\n",
       "      <td>0.806959</td>\n",
       "      <td>-0.571339</td>\n",
       "      <td>0.287627</td>\n",
       "      <td>-0.263162</td>\n",
       "      <td>0.709232</td>\n",
       "      <td>0.108071</td>\n",
       "      <td>-0.401908</td>\n",
       "      <td>0.043380</td>\n",
       "      <td>-0.273501</td>\n",
       "      <td>0.072295</td>\n",
       "      <td>0.506272</td>\n",
       "      <td>-0.367695</td>\n",
       "      <td>1.120385</td>\n",
       "      <td>-0.514067</td>\n",
       "      <td>-0.115275</td>\n",
       "      <td>0.347360</td>\n",
       "      <td>-0.484392</td>\n",
       "      <td>-0.455347</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>-0.032439</td>\n",
       "      <td>1.295160</td>\n",
       "      <td>0.219009</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.003749</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>-0.003683</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.015345</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>0.004091</td>\n",
       "      <td>1.652198</td>\n",
       "      <td>-0.047345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PTAGE  PTGENDER  PTEDUCAT  PTRACCAT_Asian  PTRACCAT_Black  PTRACCAT_Hawaiian/Other_PI  PTRACCAT_More_than_one  PTRACCAT_Unknown  PTRACCAT_White  PTETHCAT_Not_Hisp/Latino  PTMARRY_Married  PTMARRY_Never_married  PTMARRY_Widowed     APOE4  CSF_ABETA   CSF_TAU  CSF_PTAU       FDG  FDG_slope      AV45  AV45_slope    ADAS13  ADAS13_slope      MMSE  MMSE_slope  RAVLT_immediate  RAVLT_immediate_slope  RAVLT_learning  RAVLT_learning_slope  RAVLT_forgetting  RAVLT_forgetting_slope  \\\n",
       "0  2.208480         0 -2.852257               0               0                           0                       0                 0               1                         1                0                      0                1 -0.823084  -1.421715  1.137347 -0.321586 -0.833130   0.811782  1.257634    0.023763  2.140034      0.236256 -2.742373   -0.471665        -0.010026              -0.001926        0.003844             -0.000699          0.028414               -0.018332   \n",
       "1  0.759714         1  1.376909               0               0                           0                       0                 0               1                         1                1                      0                0  0.703236  -0.567568 -0.086517 -0.128000  0.602793   2.864460  1.903103    1.199280 -0.210782     -0.364551  0.586715    0.595629         0.202187              -0.189405        0.884055             -0.907178         -0.118596                1.750365   \n",
       "2 -0.257208         0  0.607970               0               0                           0                       0                 0               1                         1                1                      0                0 -0.823084  -0.775573 -1.038657 -0.920353  0.572072   0.019993  0.547072    0.023763 -0.798486     -0.710060  0.586715    0.463914         1.183334               1.011032        0.884055             -0.590705          0.280068               -1.497230   \n",
       "3 -0.521887         0 -0.160970               0               0                           0                       0                 0               1                         1                0                      0                0 -0.823084  -0.445863  0.222762  0.322200  0.441716   0.850276 -0.548541   -0.049974 -0.994388     -0.282924  0.586715    0.519651         0.730497               0.190865       -0.546632              0.454839         -0.915923                1.164759   \n",
       "4  1.121905         1 -0.160970               0               0                           0                       0                 0               1                         1                0                      0                1 -0.823084  -1.521292  0.516578  0.056582  0.613315   1.113319  1.781157    0.338415 -0.602585     -0.189067  0.586715    0.476561        -0.099704               0.488832       -0.546632              0.189374         -0.118596                0.780831   \n",
       "\n",
       "   RAVLT_perc_forgetting  RAVLT_perc_forgetting_slope      MOCA  MOCA_slope  EcogPtMem  EcogPtMem_slope  EcogPtLang  EcogPtLang_slope  EcogPtVisspat  EcogPtVisspat_slope  EcogPtPlan  EcogPtPlan_slope  EcogPtOrgan  EcogPtOrgan_slope  EcogPtDivatt  EcogPtDivatt_slope  EcogSPMem  EcogSPMem_slope  EcogSPLang  EcogSPLang_slope  EcogSPVisspat  EcogSPVisspat_slope  EcogSPPlan  EcogSPPlan_slope  EcogSPOrgan  EcogSPOrgan_slope  EcogSPDivatt  EcogSPDivatt_slope       FAQ  FAQ_slope  Ventricles  \\\n",
       "0               0.019484                    -0.012471 -0.003150   -0.004052  -0.827740         1.136728   -0.981277         -0.002156      -0.715234            -0.030329   -0.713577         -0.002410    -0.843488          -0.007633     -1.076964           -0.016544   1.929076        -0.088208   -0.844339          0.954666       1.066767             1.048123    2.524016          0.060641     2.287022           0.021430      2.330806           -0.631242  2.934694   0.124173   -0.205919   \n",
       "1              -0.556812                     1.770545  0.227228   -0.044392  -0.827740        -0.229483   -1.153377         -0.000458      -0.444998            -0.101471   -0.713577         -0.065235    -0.843488          -0.064451     -1.076964           -0.072474   0.573694        -0.738447   -0.106940         -1.151805       0.487637            -1.687565    1.262926         -2.477232     0.148179          -1.526413      0.971872           -1.384981 -0.484392  -0.424658   -0.053348   \n",
       "2              -0.495770                    -1.722958  0.452935    0.701979  -0.530055        -0.049247   -0.464974         -0.085273      -0.715234            -0.033968   -0.713577         -0.065235    -0.018265           0.055382     -0.053520            0.069781  -1.052764         0.042214   -0.844339         -0.077852      -0.670610            -0.211993   -0.754818          0.111691    -0.824015          -0.212459     -0.930635            0.246899 -0.647205  -0.415515   -0.294977   \n",
       "3              -1.032937                     0.965802  0.678642    0.204009   0.387783         0.167724    0.223444          0.375527      -0.174781             0.049689   -0.007973          0.191936     0.531879           0.404523      1.652218            0.250482  -0.103997        -0.010217   -0.844339         -0.186429      -0.670610            -0.271617   -0.754818         -0.343706    -0.824015          -0.076909     -0.115275            0.065754 -0.484392  -0.473929   -0.005181   \n",
       "4              -0.160039                     0.662249 -0.449892    0.202553  -0.133156         0.119957   -0.809176          0.038450      -0.444998            -0.146690   -0.360775         -0.127391     0.806959          -0.571339      0.287627           -0.263162   0.709232         0.108071   -0.401908          0.043380      -0.273501             0.072295    0.506272         -0.367695     1.120385          -0.514067     -0.115275            0.347360 -0.484392  -0.455347   -0.005181   \n",
       "\n",
       "   Ventricles_slope  Hippocampus  Hippocampus_slope  WholeBrain  WholeBrain_slope  Entorhinal  Entorhinal_slope  Fusiform  Fusiform_slope   MidTemp  MidTemp_slope       ICV  ICV_slope  \n",
       "0          0.381384    -1.351616           0.022285   -1.761500         -0.567555   -0.820814         -1.269796 -1.426968        0.156847 -2.102069      -0.192827 -1.574482   0.093937  \n",
       "1         -0.251084     0.483644          -0.306894   -0.134464         -0.028641   -0.070387          0.188014  0.721399       -0.067438  0.019784       0.506511 -0.489132  -0.265646  \n",
       "2         -0.883418    -1.104796           0.020857   -1.300396          0.310720    0.456478         -0.560840  0.292776        0.016824 -0.650452       0.224140 -1.239633  -0.014198  \n",
       "3         -0.032439     0.013960           0.020857   -0.000094         -0.003749    0.006635         -0.003683  0.010325        0.015345  0.018697       0.004091 -0.005136   0.004314  \n",
       "4         -0.032439     1.295160           0.219009   -0.000094         -0.003749    0.006635         -0.003683  0.010325        0.015345  0.018697       0.004091  1.652198  -0.047345  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train_std.shape)\n",
    "X_train_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best parameters\n",
    "cv_fold = KFold(n_splits=3, shuffle=True, random_state=9001)\n",
    "parameters = {'alpha': [1e-3, 1e-2, 1e-1, 0.3, 1, 3, 1e1, 1e2, 1e3],\n",
    "              'hidden_layer_sizes': [(50), (100), (200), (500), \n",
    "                                     (50, 10), (50, 25), \n",
    "                                     (100, 10)]}\n",
    "mlp = MLPClassifier(solver='lbfgs', activation='logistic', random_state=9001)\n",
    "mlp_cv = GridSearchCV(mlp, parameters, cv=cv_fold)\n",
    "mlp_cv.fit(X_train_std, y_train)\n",
    "best_score = np.argmax(mlp_cv.cv_results_['mean_test_score'])\n",
    "result = mlp_cv.cv_results_['params'][best_score]\n",
    "a = result['alpha']\n",
    "hidden_layer = result['hidden_layer_sizes']\n",
    "mlp = MLPClassifier(solver='lbfgs', activation='logistic', random_state=9001,\n",
    "                    alpha = a, hidden_layer_sizes=hidden_layer)\n",
    "mlp = mlp.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters\n",
      "L2 penalty parameter:  3\n",
      "Hidden Layer Sizes:  200\n",
      "\n",
      "-----------------\n",
      "\n",
      "Training accuracy:  0.890499194847\n",
      "Test accuracy:  0.827160493827\n",
      "\n",
      "-----------------\n",
      "\n",
      "Test Confusion Matrix: \n",
      "[[29 13  0]\n",
      " [ 7 83  3]\n",
      " [ 0  5 22]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal parameters\")\n",
    "print(\"L2 penalty parameter: \", a)\n",
    "print(\"Hidden Layer Sizes: \", hidden_layer)\n",
    "print('\\n-----------------\\n')\n",
    "print(\"Training accuracy: \", mlp.score(X_train_std, y_train))\n",
    "print(\"Test accuracy: \", mlp.score(X_test_std, y_test))\n",
    "print('\\n-----------------\\n')\n",
    "print('Test Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, mlp.predict(X_test_std)))\n",
    "nn_score = score(mlp, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------\n",
      "\n",
      "Training accuracy:  0.998389694042\n",
      "Test accuracy:  0.802469135802\n",
      "\n",
      "-----------------\n",
      "\n",
      "Test Confusion Matrix: \n",
      "[[20 22  0]\n",
      " [ 5 85  3]\n",
      " [ 0  2 25]]\n"
     ]
    }
   ],
   "source": [
    "# random forest to compare with\n",
    "rf_best = RandomForestClassifier(n_estimators=32, max_depth=12, random_state=9001)\n",
    "rf_best.fit(X_train, y_train)\n",
    "rf_score = score(rf_best, X_train, y_train, X_test, y_test)\n",
    "print('\\n-----------------\\n')\n",
    "print(\"Training accuracy: \", rf_best.score(X_train, y_train))\n",
    "print(\"Test accuracy: \", rf_best.score(X_test, y_test))\n",
    "print('\\n-----------------\\n')\n",
    "print('Test Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, rf_best.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neural Network</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train accuracy</th>\n",
       "      <td>0.890499</td>\n",
       "      <td>0.998390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test accuracy</th>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.802469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test accuracy CN</th>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test accuracy CI</th>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.913978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test accuracy AD</th>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Neural Network  Random Forest\n",
       "Train accuracy          0.890499       0.998390\n",
       "Test accuracy           0.827160       0.802469\n",
       "Test accuracy CN        0.690476       0.476190\n",
       "Test accuracy CI        0.892473       0.913978\n",
       "Test accuracy AD        0.814815       0.925926"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame({\"Neural Network\": nn_score,\n",
    "                         \"Random Forest\": rf_score})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The optimal hidden layer size is 1 hidden layer with 200 neurons. We need a l2-regularization term with value 3 to achieve the best accuracy.\n",
    "\n",
    "The overall test accuracy of neural networks is better than that of the best random forest in the previous model comparison section. It also has a significantly higher accuracy on `CN`, while its accuracy on `CI` and `AD` is slightly lower. We would say that neural networks belong to the group of Type III classifiers discussed in the previous section. It is also a very promising model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
